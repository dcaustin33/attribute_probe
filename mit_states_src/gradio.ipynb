{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!pip install -qq -U diffusers==0.11.1 transformers ftfy gradio accelerate\n",
    "\n",
    "import inspect\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import PIL\n",
    "import gradio as gr\n",
    "from diffusers import StableDiffusionInpaintPipelineLegacy\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "device = \"cuda\"\n",
    "model_path = '/home/ec2-user/attribute_probe/mit_states_src/training_scripts/inversion/individual_token_per_image/coral/outputs_solo_texture_model'\n",
    "\n",
    "pipe = StableDiffusionInpaintPipelineLegacy.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/.local/lib/python3.9/site-packages/diffusers/__init__.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import diffusers\n",
    "diffusers.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = PIL.Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = PIL.Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "def predict(dict, prompt):\n",
    "  image =  dict['image'].convert(\"RGB\").resize((512, 512))\n",
    "  # mask_image = dict['mask'].convert(\"RGB\").resize((512, 512))\n",
    "  # mask_image.save(\"mask.png\")\n",
    "  print('USING SAVED MASK')\n",
    "  mask_image = PIL.Image.open(\"mask.png\").convert(\"RGB\").resize((512, 512))\n",
    "  images = pipe(prompt=prompt,  num_inference_steps=100, guidance_scale=12.5, image=image, mask_image=mask_image, num_images_per_prompt = 4).images\n",
    "  return(images[0], images[1], images[2], images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://a013357b72c8e5435d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a013357b72c8e5435d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SAVED MASK\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859055ec0d7f46eda8584ab68d4a5c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    predict,\n",
    "    title = 'Stable Diffusion In-Painting',\n",
    "    inputs=[\n",
    "        gr.Image(source = 'upload', tool = 'sketch', type = 'pil'),\n",
    "        gr.Textbox(label = 'prompt')\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Image(), gr.Image(), gr.Image(), gr.Image()\n",
    "        ]\n",
    ").launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
