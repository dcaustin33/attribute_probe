{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming in training loop True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from mit_states_dataset import MIT_states\n",
    "from torch.utils.data import DataLoader\n",
    "dataset_args = {\n",
    "                 'root': '../../data',\n",
    "                 'transformation': True,\n",
    "                 'crop_size': 224,\n",
    "                 'brightness': 0.4, \n",
    "                 'contrast': 0.4, \n",
    "                 'saturation': .2, \n",
    "                 'hue': .1, \n",
    "                 'color_jitter_prob': .4, \n",
    "                 'gray_scale_prob': 0.2, \n",
    "                 'horizontal_flip_prob': 0.5, \n",
    "                 'gaussian_prob': .5, \n",
    "                 'min_scale': 0.6, \n",
    "                 'max_scale': 0.95}\n",
    "train_dataset = MIT_states(dataset_args['root'], dataset_args, download=False, transformation=dataset_args['transformation'])\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=3,\n",
    "    num_workers=1,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'adj',\n",
       " 1: 'ancient',\n",
       " 2: 'barren',\n",
       " 3: 'bent',\n",
       " 4: 'blunt',\n",
       " 5: 'bright',\n",
       " 6: 'broken',\n",
       " 7: 'browned',\n",
       " 8: 'brushed',\n",
       " 9: 'burnt',\n",
       " 10: 'caramelized',\n",
       " 11: 'chipped',\n",
       " 12: 'clean',\n",
       " 13: 'clear',\n",
       " 14: 'closed',\n",
       " 15: 'cloudy',\n",
       " 16: 'cluttered',\n",
       " 17: 'coiled',\n",
       " 18: 'cooked',\n",
       " 19: 'cored',\n",
       " 20: 'cracked',\n",
       " 21: 'creased',\n",
       " 22: 'crinkled',\n",
       " 23: 'crumpled',\n",
       " 24: 'crushed',\n",
       " 25: 'curved',\n",
       " 26: 'cut',\n",
       " 27: 'damp',\n",
       " 28: 'dark',\n",
       " 29: 'deflated',\n",
       " 30: 'dented',\n",
       " 31: 'diced',\n",
       " 32: 'dirty',\n",
       " 33: 'draped',\n",
       " 34: 'dry',\n",
       " 35: 'dull',\n",
       " 36: 'empty',\n",
       " 37: 'engraved',\n",
       " 38: 'eroded',\n",
       " 39: 'fallen',\n",
       " 40: 'filled',\n",
       " 41: 'foggy',\n",
       " 42: 'folded',\n",
       " 43: 'frayed',\n",
       " 44: 'fresh',\n",
       " 45: 'frozen',\n",
       " 46: 'full',\n",
       " 47: 'grimy',\n",
       " 48: 'heavy',\n",
       " 49: 'huge',\n",
       " 50: 'inflated',\n",
       " 51: 'large',\n",
       " 52: 'lightweight',\n",
       " 53: 'loose',\n",
       " 54: 'mashed',\n",
       " 55: 'melted',\n",
       " 56: 'modern',\n",
       " 57: 'moldy',\n",
       " 58: 'molten',\n",
       " 59: 'mossy',\n",
       " 60: 'muddy',\n",
       " 61: 'murky',\n",
       " 62: 'narrow',\n",
       " 63: 'new',\n",
       " 64: 'old',\n",
       " 65: 'open',\n",
       " 66: 'painted',\n",
       " 67: 'peeled',\n",
       " 68: 'pierced',\n",
       " 69: 'pressed',\n",
       " 70: 'pureed',\n",
       " 71: 'raw',\n",
       " 72: 'ripe',\n",
       " 73: 'ripped',\n",
       " 74: 'rough',\n",
       " 75: 'ruffled',\n",
       " 76: 'runny',\n",
       " 77: 'rusty',\n",
       " 78: 'scratched',\n",
       " 79: 'sharp',\n",
       " 80: 'shattered',\n",
       " 81: 'shiny',\n",
       " 82: 'short',\n",
       " 83: 'sliced',\n",
       " 84: 'small',\n",
       " 85: 'smooth',\n",
       " 86: 'spilled',\n",
       " 87: 'splintered',\n",
       " 88: 'squished',\n",
       " 89: 'standing',\n",
       " 90: 'steaming',\n",
       " 91: 'straight',\n",
       " 92: 'sunny',\n",
       " 93: 'tall',\n",
       " 94: 'thawed',\n",
       " 95: 'thick',\n",
       " 96: 'thin',\n",
       " 97: 'tight',\n",
       " 98: 'tiny',\n",
       " 99: 'toppled',\n",
       " 100: 'torn',\n",
       " 101: 'unpainted',\n",
       " 102: 'unripe',\n",
       " 103: 'upright',\n",
       " 104: 'verdant',\n",
       " 105: 'viscous',\n",
       " 106: 'weathered',\n",
       " 107: 'wet',\n",
       " 108: 'whipped',\n",
       " 109: 'wide',\n",
       " 110: 'wilted',\n",
       " 111: 'windblown',\n",
       " 112: 'winding',\n",
       " 113: 'worn',\n",
       " 114: 'wrinkled',\n",
       " 115: 'young'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.clip.modeling_clip import *\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
    "\n",
    "class CLIPTextTransformer(nn.Module):\n",
    "    def __init__(self, config: CLIPTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "        self.embeddings = CLIPTextEmbeddings(config)\n",
    "        self.encoder = CLIPEncoder(config)\n",
    "        self.final_layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(CLIP_TEXT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPTextConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"You have to specify either input_ids\")\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
    "        print(hidden_states.shape)\n",
    "\n",
    "        bsz, seq_len = input_shape\n",
    "        # CLIP's text model uses causal mask, prepare it here.\n",
    "        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n",
    "        causal_attention_mask = self._build_causal_attention_mask(bsz, seq_len, hidden_states.dtype).to(\n",
    "            hidden_states.device\n",
    "        )\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, hidden_states.dtype)\n",
    "\n",
    "        print(hidden_states.shape)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        last_hidden_state = self.final_layer_norm(last_hidden_state)\n",
    "\n",
    "        # text_embeds.shape = [batch_size, sequence_length, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14\n",
    "        pooled_output = last_hidden_state[\n",
    "            torch.arange(last_hidden_state.shape[0]), input_ids.to(torch.int).argmax(dim=-1)\n",
    "        ]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def _build_causal_attention_mask(self, bsz, seq_len, dtype):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n",
    "        mask.fill_(torch.tensor(torch.finfo(dtype).min))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        mask = mask.unsqueeze(1)  # expand mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class newCLIPTextTransformer(CLIPTextTransformer):\n",
    "    def __init__(self, old_text_transformer):\n",
    "        super().__init__(old_text_transformer.config)\n",
    "        self.embeddings = torch.nn.Embedding(100, 512)\n",
    "        self.old_text_transformer = old_text_transformer\n",
    "        self.get_all = torch.LongTensor([i for i in range(100)]).cuda()\n",
    "        self.first_dim = 10\n",
    "        self.second_dim = 10\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        \n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"You have to specify either input_ids\")\n",
    "        input_ids = torch.ones(self.first_dim, self.second_dim).cuda()\n",
    "        attention_mask = torch.ones(self.first_dim, self.second_dim).cuda()\n",
    "        input_shape = (self.first_dim, self.second_dim)\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        \n",
    "\n",
    "        bsz, seq_len = input_shape\n",
    "        hidden_states = self.embeddings(self.get_all).view(self.first_dim, self.second_dim, -1)\n",
    "        \n",
    "        # CLIP's text model uses causal mask, prepare it here.\n",
    "        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n",
    "        causal_attention_mask = self.old_text_transformer._build_causal_attention_mask(bsz, seq_len, hidden_states.dtype).to(\n",
    "            hidden_states.device\n",
    "        )\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, hidden_states.dtype)\n",
    "\n",
    "        encoder_outputs = self.old_text_transformer.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        last_hidden_state = self.old_text_transformer.final_layer_norm(last_hidden_state)\n",
    "\n",
    "        # text_embeds.shape = [batch_size, sequence_length, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14\n",
    "        pooled_output = last_hidden_state[\n",
    "            torch.arange(last_hidden_state.shape[0]), input_ids.to(torch.int).argmax(dim=-1)\n",
    "        ]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "images = torch.randn(2, 3, 224, 224)\n",
    "text = ['g '*10,] * 10\n",
    "inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n",
    "for i in inputs:\n",
    "    inputs[i] = inputs[i].cuda()\n",
    "inputs['pixel_values'] = images.cuda()\n",
    "\n",
    "text_transformer = CLIPTextTransformer(clip.config.text_config)\n",
    "text_transformer = text_transformer.cuda()\n",
    "new_transformer = newCLIPTextTransformer(text_transformer)\n",
    "new_transformer = new_transformer.cuda()\n",
    "del inputs['pixel_values']\n",
    "out = new_transformer(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
