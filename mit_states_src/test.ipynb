{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming in training loop False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from download_mit_states import download_mit_states\n",
    "\n",
    "\n",
    "class MIT_states(Dataset):\n",
    "    directory = 'release_dataset'\n",
    "    url = 'http://wednesday.csail.mit.edu/joseph_result/state_and_transformation/release_dataset.zip'\n",
    "\n",
    "    def __init__(self, root, args, train=True, loader=default_loader, download=False, transformation = True):\n",
    "        if download:\n",
    "            download_mit_states(self.url, root, self.directory)\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.train = train\n",
    "        self.num_attributes = 312\n",
    "        self.loader = loader\n",
    "\n",
    "\n",
    "        nouns = defaultdict(list)\n",
    "        adjectives = defaultdict(list)\n",
    "        concat = defaultdict(list)\n",
    "        images = os.listdir(os.path.join(self.root, self.directory, 'images'))\n",
    "        all_items = []\n",
    "        for i, val in enumerate(images):\n",
    "            if val in ['.DS_Store', '._.DS_Store'] or val[0] == '.': continue\n",
    "            old_val = val\n",
    "            val = val.split()\n",
    "            \n",
    "            if val[0] not in adjectives:\n",
    "                adjectives[val[0]] = len(adjectives.keys())\n",
    "            if val[1] not in nouns:\n",
    "                nouns[val[1]] = len(nouns.keys())\n",
    "            if old_val not in concat:\n",
    "                concat[old_val] = len(concat.keys())\n",
    "\n",
    "            items = os.listdir(os.path.join(self.root, self.directory, 'images', old_val))\n",
    "            for item in items:\n",
    "                if item[0] == '.': continue\n",
    "                labels = {}\n",
    "                labels['adjective'] = adjectives[val[0]]\n",
    "                labels['noun'] = nouns[val[1]]\n",
    "                labels['concat'] = concat[old_val]\n",
    "                all_items.append([os.path.join(self.root, self.directory, 'images', old_val, item), labels])\n",
    "\n",
    "        self.all_items = all_items\n",
    "        self.adjectives = dict([(value, key) for key, value in adjectives.items()])\n",
    "        self.nouns = dict([(value, key) for key, value in nouns.items()])\n",
    "        self.concat = dict([(value, key) for key, value in concat.items()])\n",
    "        self.transformation = transformation\n",
    "\n",
    "        self.initial_transforms = transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop((args['crop_size'], args['crop_size']), scale=(args['min_scale'], args['max_scale']),\n",
    "                                            interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "                                    torchvision.transforms.ToTensor()])\n",
    "\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "                            transforms.RandomApply(\n",
    "                                    [transforms.ColorJitter(args['brightness'], args['contrast'], args['saturation'], args['hue'])],\n",
    "                                    p=args['color_jitter_prob'],\n",
    "                                ),\n",
    "                            torchvision.transforms.RandomHorizontalFlip(p=args['horizontal_flip_prob']),\n",
    "                            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                std=[0.229, 0.224, 0.225])\n",
    "                            ])  \n",
    "        number_items = [i for i in range(len(self.all_items))]\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(number_items)\n",
    "        split = int(len(self.all_items) * .8)\n",
    "        if self.train:\n",
    "            self.all_items = [self.all_items[i] for i in number_items[:split]]\n",
    "        else:\n",
    "            self.all_items = [self.all_items[i] for i in number_items[split:]]\n",
    "\n",
    "        print('Transforming in training loop', self.transformation)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.all_items[idx]\n",
    "        img = self.loader(image_path)\n",
    "        img = self.initial_transforms(img)\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "\n",
    "        if self.transformation:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        batch = {}\n",
    "        batch['image'] = img\n",
    "        batch['concat_labels'] = label['concat']\n",
    "        batch['adjective_labels'] = label['adjective']\n",
    "        batch['noun_labels'] = label['noun']\n",
    "        batch['adjective_sentence'] = 'This is ' + self.adjectives[label['adjective']]\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torchvision.transforms as transforms\n",
    "    dataset_args = {\n",
    "                 'crop_size': 224,\n",
    "                 'brightness': 0.4, \n",
    "                 'contrast': 0.4, \n",
    "                 'saturation': .2, \n",
    "                 'hue': .1, \n",
    "                 'color_jitter_prob': .4, \n",
    "                 'gray_scale_prob': 0.2, \n",
    "                 'horizontal_flip_prob': 0.5, \n",
    "                 'gaussian_prob': .5, \n",
    "                 'min_scale': 0.6, \n",
    "                 'max_scale': 0.95}\n",
    "\n",
    "    dataset = MIT_states(root='../data/', args = dataset_args, train=True, download=False, transformation = False)\n",
    "    item = dataset.__getitem__(0)\n",
    "    #print(item['image'].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "for i, data in enumerate(loader): break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
