{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cub_dataset import Cub2011\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class Cub2011(Dataset):\n",
    "    base_folder = 'CUB_200_2011/images'\n",
    "    url = 'https://data.deepai.org/CUB200\\(2011\\).zip'\n",
    "    filename = 'CUB200\\(2011\\).zip'\n",
    "    tarfile = 'CUB_200_2011.tgz'\n",
    "    directory = 'CUB_200_2011'\n",
    "    tgz_md5 = '97eceeb196236b17998738112f37df78'\n",
    "\n",
    "    def __init__(self, root, args, train=True, loader=default_loader, download=False,):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        #self.transform = transform\n",
    "        self.loader = default_loader\n",
    "        self.train = train\n",
    "        self.num_attributes = 312\n",
    "\n",
    "        if download:\n",
    "            self._download()\n",
    "\n",
    "\n",
    "        #initialize the attributes\n",
    "        with open(os.path.join(root, 'CUB_200_2011', 'attributes', 'image_attribute_labels.txt'), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        attributes = defaultdict(list)\n",
    "        certainty = defaultdict(list)\n",
    "        for line in lines:\n",
    "            line = line.strip().split()\n",
    "            attributes[line[0]].append(line[2])\n",
    "            certainty[line[0]].append(line[3])\n",
    "\n",
    "        self.attributes = pd.DataFrame.from_dict(attributes, orient='index').reset_index()\n",
    "        self.certainty = pd.DataFrame.from_dict(certainty, orient='index').reset_index()\n",
    "        #certaintys 1 = not visible, 2 = guessing, 3 = probably, 4 = definite\n",
    "        \n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "                                transforms.RandomApply(\n",
    "                                    [transforms.ColorJitter(args['brightness'], args['contrast'], args['saturation'], args['hue'])],\n",
    "                                    p=args['color_jitter_prob'],\n",
    "                                ),\n",
    "                            #torchvision.transforms.Resize((224, 224)),\n",
    "                            transforms.RandomResizedCrop((args['crop_size'], args['crop_size']), scale=(args['min_scale'], args['max_scale']),\n",
    "                                    interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            torchvision.transforms.RandomHorizontalFlip(p=args['horizontal_flip_prob']),\n",
    "                            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                std=[0.229, 0.224, 0.225])\n",
    "                            ])  \n",
    "\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        images = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'images.txt'), sep=' ',\n",
    "                             names=['img_id', 'filepath'])\n",
    "        image_class_labels = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'image_class_labels.txt'),\n",
    "                                         sep=' ', names=['img_id', 'target'])\n",
    "        train_test_split = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'train_test_split.txt'),\n",
    "                                       sep=' ', names=['img_id', 'is_training_img'])\n",
    "\n",
    "        data = images.merge(image_class_labels, on='img_id')\n",
    "        self.data = data.merge(train_test_split, on='img_id')\n",
    "\n",
    "        if self.train:\n",
    "            truth = self.data.is_training_img == 1\n",
    "            self.data = self.data[truth]\n",
    "            self.attributes = self.attributes[truth]\n",
    "            self.certainty = self.certainty[truth]\n",
    "        else:\n",
    "            truth = self.data.is_training_img == 0\n",
    "            self.data = self.data[truth]\n",
    "            self.attributes = self.attributes[truth]\n",
    "            self.certainty = self.certainty[truth]\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        try:\n",
    "            self._load_metadata()\n",
    "        except Exception:\n",
    "            return False\n",
    "        for index, row in self.data.iterrows():\n",
    "            filepath = os.path.join(self.root, self.base_folder, row.filepath)\n",
    "            if not os.path.isfile(filepath):\n",
    "                print(filepath)\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        if not os.path.exists(self.root): os.mkdir(self.root)\n",
    "        os.system('wget ' + self.url)\n",
    "        os.system('unzip ' + self.filename)\n",
    "        os.system('rm ' + self.filename)\n",
    "        os.system('tar -zxf ' + self.tarfile)\n",
    "        os.system('rm ' + self.tarfile)\n",
    "        os.system('mv ' + self.directory + ' ' + self.root + '/CUB_200_2011')\n",
    "        os.system('mv attributes.txt ' + self.root + '/CUB_200_2011')\n",
    "        os.system('rm segmentations.tgz')\n",
    "\n",
    "    def get_attribute(self, idx):\n",
    "        return self.attributes[idx]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        attributes = torch.Tensor(self.attributes.iloc[idx, 1:].values.astype(np.int8))\n",
    "        certainty = torch.Tensor(self.certainty.iloc[idx, 1:].values.astype(np.int8))\n",
    "        path = os.path.join(self.root, self.base_folder, sample.filepath)\n",
    "        target = sample.target - 1  # Targets start at 1 by default, so shift to 0\n",
    "        img = self.loader(path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        batch = {}\n",
    "        batch['image'] = img\n",
    "        batch['class'] = target\n",
    "        batch['attributes'] = attributes\n",
    "        batch['certainty'] = certainty\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset_args = {\n",
    "                 'root': '../data',\n",
    "                 'crop_size': 224,\n",
    "                 'brightness': 0.4, \n",
    "                 'contrast': 0.4, \n",
    "                 'saturation': .2, \n",
    "                 'hue': .1, \n",
    "                 'color_jitter_prob': .4, \n",
    "                 'gray_scale_prob': 0.2, \n",
    "                 'horizontal_flip_prob': 0.5, \n",
    "                 'gaussian_prob': .5, \n",
    "                 'min_scale': 0.6, \n",
    "                 'max_scale': 0.95}\n",
    "\n",
    "dataset = Cub2011(dataset_args['root'], dataset_args, download=False)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32,  shuffle=False, num_workers=0)\n",
    "for i, data in enumerate(dataloader):\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "certain = 0\n",
    "for i, data in enumerate(dataloader):\n",
    "    total += data['image'].shape[0]\n",
    "    certain += (torch.sum(data['certainty'] >= 2))/312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8910)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "certain / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5205)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "certain / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch.nn as nn\n",
    "\n",
    "def create_text_prompts(root):\n",
    "    with open(root + '/CUB_200_2011/attributes.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    text_prompts = []\n",
    "    for line in lines:\n",
    "\n",
    "        #base that will be used for every image\n",
    "        start = 'The bird has a '\n",
    "\n",
    "        #get the words before seeing the descriptor\n",
    "        beginning = ''\n",
    "        seen = False\n",
    "\n",
    "\n",
    "        for i in line.split()[1].split('_'):\n",
    "\n",
    "            #:: signigifies that the attribute value is on the other side\n",
    "            if '::' in i:\n",
    "                first_half = i.split('::')[0]\n",
    "                second_half = i.split('::')[1]\n",
    "                seen = True\n",
    "\n",
    "            #if we have seen the descriptor, we are done and ( signifies that\n",
    "            if '(' in i:\n",
    "                break\n",
    "            if i != 'has':\n",
    "                if '::' in i: continue\n",
    "                if seen: second_half += ' ' + i\n",
    "                else: beginning += i + ' '\n",
    "        start += second_half + ' ' + beginning  + first_half\n",
    "        text_prompts.append(start)\n",
    "    return text_prompts\n",
    "\n",
    "text_prompts = create_text_prompts('../data/')\n",
    "\n",
    "class CLIP_text_image_concat(nn.Module):\n",
    "\n",
    "    def __init__(self, args = None):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.linear1 = nn.ModuleList([nn.Linear(1024, 1) for i in range(312)])\n",
    "        self.linear2 = nn.ModuleList([nn.Linear(768 + 512, 1) for i in range(312)])\n",
    "        self.classifier1 = nn.ModuleList([nn.Sequential(nn.Linear(1024, 312), nn.ReLU(), nn.BatchNorm1d(312), nn.Linear(312, 1)) for i in range(312)])\n",
    "        self.classifier2 = nn.ModuleList([nn.Sequential(nn.Linear(768 + 512, 312), nn.ReLU(), nn.BatchNorm1d(312), nn.Linear(312, 1)) for i in range(312)])\n",
    "\n",
    "    def forward(self, prompts, images):\n",
    "        text = prompts\n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\", padding=True)\n",
    "        for i in inputs:\n",
    "            inputs[i] = inputs[i].cuda()\n",
    "        inputs['pixel_values'] = images.cuda()\n",
    "        outputs = self.clip(**inputs)\n",
    "\n",
    "        image_embed = outputs.image_embeds\n",
    "        image_out = outputs.vision_model_output['pooler_output']\n",
    "        text_embed = outputs.text_embeds\n",
    "        text_out = outputs.text_model_output['pooler_output']\n",
    "\n",
    "        classifications = []\n",
    "\n",
    "        for i in range(text_embed.size(0)):\n",
    "            new_text_embed = text_embed[i].unsqueeze(0).repeat(image_embed.size(0), 1)\n",
    "            new_text_out = text_out[i].unsqueeze(0).repeat(image_out.size(0), 1)\n",
    "            final_embed = torch.cat((image_embed, new_text_embed), dim=1)\n",
    "            final_out = torch.cat((image_out, new_text_out), dim=1)\n",
    "\n",
    "            lin1 = self.linear1[i](final_embed)\n",
    "            class1 = self.classifier1[i](final_embed)\n",
    "\n",
    "            lin2 = self.linear2[i](final_out)\n",
    "            class2 = self.classifier2[i](final_out)\n",
    "            print(class2.size())\n",
    "\n",
    "            inter_class = [lin1, lin2, class1, class2]\n",
    "            if i == 0:\n",
    "                classifications = inter_class\n",
    "\n",
    "            else:\n",
    "                classifications[0] = torch.cat((classifications[0], inter_class[0]), dim=1)\n",
    "                classifications[1] = torch.cat((classifications[1], inter_class[1]), dim=1)\n",
    "                classifications[2] = torch.cat((classifications[2], inter_class[2]), dim=1)\n",
    "                classifications[3] = torch.cat((classifications[3], inter_class[3]), dim=1)\n",
    "\n",
    "        return classifications, outputs.logits_per_image\n",
    "\n",
    "model = CLIP_text_image_concat()\n",
    "model.cuda()\n",
    "images, attributes, certainty = data['image'].cuda(), data['attributes'].cuda(), data['certainty'].cuda()\n",
    "\n",
    "truth = certainty >= 3\n",
    "classification_out, clip_image_logits = model(text_prompts, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 312])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indicates where a multivalued attribute starts and ends\n",
    "attributes = data['attributes'][0]\n",
    "certainty = data['certainty'][0]\n",
    "indices = [9, 24, 39, 54, 58, 73, 79, 94, 105, 120, 135, 149, 152, 167, 182, 197, 212, 217, 222, 236, 240, 244, 248, 263, 278, 293, 308, 312]\n",
    "multi = [0 for i in range(len(indices))]\n",
    "start = 0\n",
    "for f in range(len(data['attributes'])):\n",
    "    attributes = data['attributes'][f]\n",
    "    certainty = data['certainty'][f]\n",
    "    start = 0\n",
    "    for val, i in enumerate(indices):\n",
    "        if certainty[start] >= 3:\n",
    "            if attributes[start:i - 1].sum() > 1:\n",
    "                #print('multivalued attribute', start, i)\n",
    "                multi[val] += 1\n",
    "        start = i\n",
    "print(multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
