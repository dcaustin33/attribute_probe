{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dandelin/vilt-b32-mlm were not used when initializing ViltModel: ['mlm_score.transform.LayerNorm.weight', 'mlm_score.transform.LayerNorm.bias', 'mlm_score.transform.dense.bias', 'mlm_score.decoder.weight', 'mlm_score.transform.dense.weight', 'mlm_score.bias']\n",
      "- This IS expected if you are initializing ViltModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViltModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViltProcessor, ViltModel, ViltFeatureExtractor\n",
    "from PIL import Image\n",
    "import requests\n",
    "from cub_with_attribute import Cub2011\n",
    "\n",
    "\n",
    "\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "model = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\").cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using unnormalized data\n",
      "in transform\n",
      "in transform\n",
      "in transform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in transform\n",
      "in transform\n",
      "in transform\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "dataset_args = {\n",
    "                 'root': '../data/',\n",
    "                 'attribute_idx_amount': 1,\n",
    "                 'normalize': False,\n",
    "                 'crop_size': 224,\n",
    "                 'brightness': 0.4, \n",
    "                 'contrast': 0.4, \n",
    "                 'saturation': .2, \n",
    "                 'hue': .1, \n",
    "                 'color_jitter_prob': .4, \n",
    "                 'gray_scale_prob': 0.2, \n",
    "                 'horizontal_flip_prob': 0.5, \n",
    "                 'gaussian_prob': .5, \n",
    "                 'min_scale': 0.6, \n",
    "                 'max_scale': 0.95}\n",
    "train_dataset = Cub2011(dataset_args['root'], dataset_args, download=False, normalize = dataset_args['normalize'])\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=1,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "for i, data in enumerate(dataloader): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extract\n",
    "import torch\n",
    "import numpy as np\n",
    "images = [i for i in data['image']]\n",
    "feature_extractor = ViltFeatureExtractor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "out = feature_extractor(images, return_tensors=\"pt\", padding = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('../data/' + '/CUB_200_2011/attributes.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "text_prompts = []\n",
    "for line in lines:\n",
    "\n",
    "    #base that will be used for every image\n",
    "    start = 'The bird has a '\n",
    "\n",
    "    #get the words before seeing the descriptor\n",
    "    beginning = ''\n",
    "    seen = False\n",
    "\n",
    "\n",
    "    for i in line.split()[1].split('_'):\n",
    "\n",
    "        #:: signigifies that the attribute value is on the other side\n",
    "        if '::' in i:\n",
    "            first_half = i.split('::')[0]\n",
    "            second_half = i.split('::')[1]\n",
    "            seen = True\n",
    "\n",
    "        #if we have seen the descriptor, we are done and ( signifies that\n",
    "        if '(' in i:\n",
    "            break\n",
    "        if i != 'has':\n",
    "            if '::' in i: continue\n",
    "            if seen: second_half += ' ' + i\n",
    "            else: beginning += i + ' '\n",
    "    start += second_half + ' ' + beginning  + first_half\n",
    "    text_prompts.append(start)\n",
    "\n",
    "text_prompts = np.array(text_prompts)\n",
    "correct_prompts = list(text_prompts[data['attribute_idx']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.1462, 2.1462, 2.1462,  ..., 2.0948, 2.0948, 2.0948],\n",
       "         [2.0263, 2.0263, 2.0434,  ..., 1.8722, 1.8722, 1.8722],\n",
       "         [2.0948, 2.0948, 2.0948,  ..., 1.9407, 1.9578, 1.9578],\n",
       "         ...,\n",
       "         [1.5297, 1.4783, 1.5125,  ..., 1.3070, 1.2557, 1.1872],\n",
       "         [1.5639, 1.5810, 1.5810,  ..., 1.3070, 1.2385, 1.1700],\n",
       "         [1.6324, 1.6324, 1.6324,  ..., 1.2899, 1.2385, 1.2043]],\n",
       "\n",
       "        [[2.3235, 2.3235, 2.3235,  ..., 2.2710, 2.2535, 2.2535],\n",
       "         [2.2010, 2.1835, 2.2010,  ..., 2.0434, 2.0609, 2.0609],\n",
       "         [2.2360, 2.2360, 2.2535,  ..., 2.1310, 2.1485, 2.1485],\n",
       "         ...,\n",
       "         [1.6933, 1.6758, 1.7108,  ..., 1.5007, 1.4482, 1.3782],\n",
       "         [1.7283, 1.7458, 1.7283,  ..., 1.5007, 1.4307, 1.3606],\n",
       "         [1.7808, 1.7808, 1.7808,  ..., 1.4832, 1.4307, 1.3957]],\n",
       "\n",
       "        [[2.5529, 2.5529, 2.5529,  ..., 2.5006, 2.5006, 2.5006],\n",
       "         [2.4831, 2.5006, 2.5180,  ..., 2.3611, 2.3611, 2.3611],\n",
       "         [2.5529, 2.5529, 2.5703,  ..., 2.4308, 2.4483, 2.4483],\n",
       "         ...,\n",
       "         [2.1346, 2.0997, 2.1694,  ..., 1.9777, 1.9254, 1.8557],\n",
       "         [2.1694, 2.1694, 2.1868,  ..., 1.9777, 1.9080, 1.8383],\n",
       "         [2.2391, 2.2391, 2.2566,  ..., 1.9603, 1.9080, 1.8731]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1996, 4743, 2038, 1037, 4462, 3356, 5725, 3609,  102],\n",
       "        [ 101, 1996, 4743, 2038, 1037, 2829, 3759, 3609,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), 'pixel_values': tensor([[[[-0.7098, -0.7098, -0.7098,  ..., -0.9451, -0.9451, -0.9451],\n",
       "          [-0.8118, -0.8118, -0.8118,  ..., -0.3333, -0.3333, -0.3333],\n",
       "          [-0.9686, -0.9686, -0.9686,  ...,  0.6941,  0.6941,  0.6941],\n",
       "          ...,\n",
       "          [ 0.1216,  0.1373,  0.1608,  ..., -0.5451, -0.6314, -0.6784],\n",
       "          [ 0.2078,  0.2157,  0.2314,  ..., -0.5451, -0.6000, -0.6314],\n",
       "          [ 0.2627,  0.2627,  0.2627,  ..., -0.5373, -0.5843, -0.6000]],\n",
       "\n",
       "         [[-0.3569, -0.3490, -0.3490,  ..., -0.4824, -0.4824, -0.4824],\n",
       "          [-0.4510, -0.4588, -0.4667,  ..., -0.6471, -0.6471, -0.6471],\n",
       "          [-0.6078, -0.6235, -0.6471,  ..., -0.8902, -0.8902, -0.8902],\n",
       "          ...,\n",
       "          [ 0.4510,  0.4667,  0.4902,  ..., -0.1608, -0.2549, -0.3020],\n",
       "          [ 0.5216,  0.5294,  0.5373,  ..., -0.1608, -0.2235, -0.2549],\n",
       "          [ 0.5608,  0.5608,  0.5608,  ..., -0.1529, -0.2078, -0.2235]],\n",
       "\n",
       "         [[ 0.0902,  0.0902,  0.0902,  ...,  0.0039,  0.0039,  0.0039],\n",
       "          [ 0.0353,  0.0353,  0.0431,  ..., -0.1137, -0.1137, -0.1137],\n",
       "          [-0.0510, -0.0431, -0.0196,  ..., -0.2941, -0.2941, -0.2941],\n",
       "          ...,\n",
       "          [-0.6706, -0.6706, -0.6706,  ...,  0.7961,  0.7020,  0.6549],\n",
       "          [-0.5843, -0.5843, -0.5843,  ...,  0.8039,  0.7333,  0.7020],\n",
       "          [-0.5373, -0.5373, -0.5373,  ...,  0.8039,  0.7490,  0.7333]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2157,  0.2157,  0.2157,  ...,  0.0824,  0.0824,  0.0824],\n",
       "          [ 0.2157,  0.2157,  0.2157,  ...,  0.0824,  0.0824,  0.0824],\n",
       "          [ 0.2157,  0.2157,  0.2157,  ...,  0.0824,  0.0824,  0.0824],\n",
       "          ...,\n",
       "          [ 0.1686,  0.1608,  0.1373,  ...,  0.2078,  0.1843,  0.1765],\n",
       "          [ 0.2157,  0.2078,  0.1843,  ...,  0.1843,  0.2235,  0.2471],\n",
       "          [ 0.2471,  0.2392,  0.2157,  ...,  0.1686,  0.2471,  0.2863]],\n",
       "\n",
       "         [[ 0.4431,  0.4431,  0.4431,  ...,  0.3412,  0.3412,  0.3412],\n",
       "          [ 0.4431,  0.4431,  0.4431,  ...,  0.3412,  0.3412,  0.3412],\n",
       "          [ 0.4431,  0.4431,  0.4431,  ...,  0.3412,  0.3412,  0.3412],\n",
       "          ...,\n",
       "          [ 0.5608,  0.5529,  0.5294,  ...,  0.5608,  0.5451,  0.5373],\n",
       "          [ 0.5922,  0.5922,  0.5843,  ...,  0.5686,  0.5922,  0.6078],\n",
       "          [ 0.6000,  0.6000,  0.6078,  ...,  0.5608,  0.6157,  0.6471]],\n",
       "\n",
       "         [[-0.7882, -0.7882, -0.7882,  ..., -0.8902, -0.8902, -0.8902],\n",
       "          [-0.7882, -0.7882, -0.7882,  ..., -0.8902, -0.8902, -0.8902],\n",
       "          [-0.7882, -0.7882, -0.7882,  ..., -0.8902, -0.8902, -0.8902],\n",
       "          ...,\n",
       "          [-0.5922, -0.6000, -0.6235,  ..., -0.4824, -0.5059, -0.5137],\n",
       "          [-0.5451, -0.5529, -0.5765,  ..., -0.4745, -0.4510, -0.4431],\n",
       "          [-0.5137, -0.5216, -0.5451,  ..., -0.4824, -0.4275, -0.4039]]]]), 'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "for i in data['image']:\n",
    "    images.append(i)\n",
    "processor(images, correct_prompts, return_tensors=\"pt\", padding = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 384, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The bird has a grey upper tail color', 'The bird has a brown throat color']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ec2-user/attribute_probe/clip_src/port_tf_pt.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224177735f6563325f736d616c6c227d/home/ec2-user/attribute_probe/clip_src/port_tf_pt.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m inputs \u001b[39m=\u001b[39m processor([image, image2], [text, text], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224177735f6563325f736d616c6c227d/home/ec2-user/attribute_probe/clip_src/port_tf_pt.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m'''for i in inputs:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224177735f6563325f736d616c6c227d/home/ec2-user/attribute_probe/clip_src/port_tf_pt.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m    inputs[i] = inputs[i].cuda()'''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224177735f6563325f736d616c6c227d/home/ec2-user/attribute_probe/clip_src/port_tf_pt.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224177735f6563325f736d616c6c227d/home/ec2-user/attribute_probe/clip_src/port_tf_pt.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m last_hidden_states \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/vilt/modeling_vilt.py:838\u001b[0m, in \u001b[0;36mViltModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, pixel_values, pixel_mask, head_mask, inputs_embeds, image_embeds, image_token_type_idx, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    836\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 838\u001b[0m embedding_output, attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    839\u001b[0m     input_ids,\n\u001b[1;32m    840\u001b[0m     attention_mask,\n\u001b[1;32m    841\u001b[0m     token_type_ids,\n\u001b[1;32m    842\u001b[0m     pixel_values,\n\u001b[1;32m    843\u001b[0m     pixel_mask,\n\u001b[1;32m    844\u001b[0m     inputs_embeds,\n\u001b[1;32m    845\u001b[0m     image_embeds,\n\u001b[1;32m    846\u001b[0m     image_token_type_idx\u001b[39m=\u001b[39;49mimage_token_type_idx,\n\u001b[1;32m    847\u001b[0m )\n\u001b[1;32m    849\u001b[0m \u001b[39m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[39m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[1;32m    851\u001b[0m extended_attention_mask: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/vilt/modeling_vilt.py:212\u001b[0m, in \u001b[0;36mViltEmbeddings.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, pixel_values, pixel_mask, inputs_embeds, image_embeds, image_token_type_idx)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    202\u001b[0m     input_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m ):\n\u001b[1;32m    211\u001b[0m     \u001b[39m# PART 1: text embeddings\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     text_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_embeddings(\n\u001b[1;32m    213\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids, token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids, inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     \u001b[39m# PART 2: patch embeddings (with interpolated position encodings)\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39mif\u001b[39;00m image_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/vilt/modeling_vilt.py:288\u001b[0m, in \u001b[0;36mTextEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    285\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[1;32m    289\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    291\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "# prepare image and text\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = \"hello world\"\n",
    "url2 = 'https://farm1.staticflickr.com/79/269579885_4c423901f9_z.jpg'\n",
    "image2 = Image.open(requests.get(url2, stream=True).raw)\n",
    "\n",
    "\n",
    "\n",
    "inputs = processor([image, image2], [text, text], return_tensors=\"pt\")\n",
    "for i in inputs:\n",
    "    inputs[i] = inputs[i].cuda()\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 2088,  102],\n",
       "        [ 101, 7592, 2088,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]]), 'pixel_values': tensor([[[[ 0.1059,  0.1294,  0.1373,  ..., -0.2000, -0.1922, -0.1922],\n",
       "          [ 0.0824,  0.1137,  0.1608,  ..., -0.2235, -0.1373, -0.2314],\n",
       "          [ 0.0980,  0.1765,  0.1529,  ..., -0.1922, -0.2078, -0.2235],\n",
       "          ...,\n",
       "          [ 0.8353,  0.8353,  0.8196,  ...,  0.4980,  0.4667,  0.4510],\n",
       "          [ 0.8118,  0.8039,  0.7882,  ...,  0.0353,  0.0667, -0.1137],\n",
       "          [ 0.8667,  0.9059,  0.7098,  ..., -0.3490, -0.4039, -0.4275]],\n",
       "\n",
       "         [[-0.8039, -0.8118, -0.8275,  ..., -0.9137, -0.8824, -0.9137],\n",
       "          [-0.8118, -0.8039, -0.7961,  ..., -0.9059, -0.8824, -0.8980],\n",
       "          [-0.7961, -0.7569, -0.7882,  ..., -0.8745, -0.8588, -0.9059],\n",
       "          ...,\n",
       "          [-0.2627, -0.2627, -0.2549,  ..., -0.5686, -0.5216, -0.5608],\n",
       "          [-0.3176, -0.3176, -0.2863,  ..., -0.7647, -0.7647, -0.8745],\n",
       "          [-0.2314, -0.1922, -0.3882,  ..., -0.8824, -0.8745, -0.8824]],\n",
       "\n",
       "         [[-0.5451, -0.4902, -0.4667,  ..., -0.6941, -0.6941, -0.7333],\n",
       "          [-0.5922, -0.6078, -0.5137,  ..., -0.7176, -0.6941, -0.7569],\n",
       "          [-0.6392, -0.4980, -0.5529,  ..., -0.7176, -0.7255, -0.7725],\n",
       "          ...,\n",
       "          [ 0.5451,  0.5373,  0.5216,  ...,  0.2314,  0.2627,  0.2235],\n",
       "          [ 0.6392,  0.6157,  0.5608,  ..., -0.3255, -0.3255, -0.4588],\n",
       "          [ 0.4667,  0.6000,  0.5451,  ..., -0.7490, -0.7176, -0.6941]]],\n",
       "\n",
       "\n",
       "        [[[-0.2392,  0.0118, -0.1294,  ...,  0.4039,  0.4510,  0.0510],\n",
       "          [ 0.0353,  0.3098,  0.1529,  ...,  0.8118,  0.8667,  0.4510],\n",
       "          [-0.0118,  0.2000,  0.1216,  ...,  0.7098,  0.8039,  0.3647],\n",
       "          ...,\n",
       "          [-0.4039, -0.3490, -0.3725,  ..., -0.4824, -0.4745, -0.6706],\n",
       "          [-0.2078, -0.2078, -0.2314,  ..., -0.4431, -0.4196, -0.6000],\n",
       "          [-0.2706, -0.2235, -0.4353,  ..., -0.6549, -0.5373, -0.6941]],\n",
       "\n",
       "         [[-0.1843,  0.0745, -0.0431,  ...,  0.4431,  0.4902,  0.0902],\n",
       "          [ 0.0980,  0.3804,  0.2235,  ...,  0.8510,  0.9059,  0.4902],\n",
       "          [ 0.0353,  0.2471,  0.1922,  ...,  0.7490,  0.8431,  0.4039],\n",
       "          ...,\n",
       "          [-0.2784, -0.1529, -0.1373,  ..., -0.0353, -0.0824, -0.3255],\n",
       "          [-0.1608, -0.0588,  0.0039,  ..., -0.0510, -0.0824, -0.3176],\n",
       "          [-0.3098, -0.1137, -0.1922,  ..., -0.3255, -0.2627, -0.4588]],\n",
       "\n",
       "         [[-0.5216, -0.3569, -0.5373,  ...,  0.4745,  0.5216,  0.1216],\n",
       "          [-0.2392, -0.0196, -0.2314,  ...,  0.8824,  0.9373,  0.5216],\n",
       "          [-0.2941, -0.0824, -0.1922,  ...,  0.7804,  0.8745,  0.4353],\n",
       "          ...,\n",
       "          [-0.4353, -0.4745, -0.6000,  ..., -0.7961, -0.7412, -0.8118],\n",
       "          [-0.3961, -0.4431, -0.4980,  ..., -0.7804, -0.7098, -0.7647],\n",
       "          [-0.6078, -0.5529, -0.7333,  ..., -0.9608, -0.8510, -0.8824]]]]), 'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
